name: Tests

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  # Fast smoke test - runs first as a gate
  smoke:
    name: üöÄ Smoke Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "poetry"

      - name: Install dependencies
        run: poetry install --with dev --no-interaction

      - name: Run smoke tests
        run: poetry run pytest tests/ -m smoke -v --tb=short --junitxml=test-results-smoke.xml

      - name: Upload smoke results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-smoke
          path: test-results-smoke.xml
          retention-days: 7

  # Parallel test matrix - runs all categories simultaneously
  test-matrix:
    name: ${{ matrix.name }}
    runs-on: ubuntu-latest
    needs: smoke  # Only run if smoke passes
    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        include:
          - name: "üîß Unit Tests"
            marker: unit
            artifact: unit
            
          - name: "üìã Compliance Tests"
            marker: compliance
            artifact: compliance
            
          - name: "üîí Security Tests"
            marker: security
            artifact: security
            
          - name: "üì¶ Integration Tests"
            marker: integration
            artifact: integration

    steps:
      - uses: actions/checkout@v4

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "poetry"

      - name: Install dependencies
        run: poetry install --with dev --no-interaction

      - name: Run tests
        run: |
          poetry run pytest tests/ -m '${{ matrix.marker }}' \
            -v \
            --tb=short \
            --junitxml=test-results-${{ matrix.artifact }}.xml

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.artifact }}
          path: test-results-*.xml
          retention-days: 7

  # Benchmark tests - separate job (optional, can be slow)
  benchmarks:
    name: ‚è±Ô∏è Benchmarks
    runs-on: ubuntu-latest
    needs: smoke
    steps:
      - uses: actions/checkout@v4

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "poetry"

      - name: Install dependencies
        run: poetry install --with dev --no-interaction

      - name: Run benchmark tests
        run: |
          poetry run pytest tests/ -m benchmark \
            -v \
            --tb=short \
            --junitxml=test-results-benchmark.xml

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-benchmark
          path: test-results-benchmark.xml
          retention-days: 7

  # Fuzz tests - separate job (slow, resource intensive)
  fuzz:
    name: üé≤ Fuzz Tests
    runs-on: ubuntu-latest
    needs: smoke
    steps:
      - uses: actions/checkout@v4

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "poetry"

      - name: Install dependencies
        run: poetry install --with dev --no-interaction

      - name: Run fuzz tests
        run: |
          poetry run pytest tests/ -m fuzz \
            -v \
            --tb=short \
            --junitxml=test-results-fuzz.xml
        timeout-minutes: 10  # Fuzz tests have a time limit

      - name: Upload fuzz results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-fuzz
          path: test-results-fuzz.xml
          retention-days: 7

  # Publish test results as PR comment
  publish-results:
    name: üìä Publish Results
    runs-on: ubuntu-latest
    needs: [smoke, test-matrix, benchmarks, fuzz]
    if: always() && github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results
          pattern: test-results-*
          merge-multiple: true

      - name: Generate Summary Markdown
        id: generate-summary
        run: |
          cat << 'EOF' > generate_summary.py
          import glob
          import xml.etree.ElementTree as ET
          import os

          # Configuration
          categories = [
              ("üöÄ Smoke Test", "smoke"),
              ("üîß Unit Tests", "unit"),
              ("üìã Compliance Tests", "compliance"),
              ("üîí Security Tests", "security"),
              ("üì¶ Integration Tests", "integration"),
              ("‚è±Ô∏è Benchmarks", "benchmark"),
              ("üé≤ Fuzz Tests", "fuzz")
          ]

          summary_lines = []
          summary_lines.append("### üß™ Test Results Overview")
          summary_lines.append("")
          summary_lines.append("| Status | Category | Tests | Time |")
          summary_lines.append("| :---: | :--- | :---: | :---: |")

          all_passed = True
          
          for name, key in categories:
              file_pattern = f"test-results/test-results-{key}.xml"
              files = glob.glob(file_pattern)
              
              if not files:
                  summary_lines.append(f"| ‚ö™ | {name} | N/A | - |")
                  continue
                  
              total_tests = 0
              total_failures = 0
              total_errors = 0
              total_skipped = 0
              total_time = 0.0
              
              try:
                  for f in files:
                      tree = ET.parse(f)
                      root = tree.getroot()
                      # Handle both testsuites and testsuite root
                      suites = root.findall('.//testsuite')
                      if not suites and root.tag == 'testsuite':
                          suites = [root]
                      
                      for suite in suites:
                          total_tests += int(suite.get('tests', 0))
                          total_failures += int(suite.get('failures', 0))
                          total_errors += int(suite.get('errors', 0))
                          total_skipped += int(suite.get('skipped', 0))
                          total_time += float(suite.get('time', 0))
                  
                  status_icon = "‚úÖ"
                  if total_failures > 0 or total_errors > 0:
                      status_icon = "‚ùå"
                      all_passed = False
                  elif total_tests == 0:
                       status_icon = "‚ö†Ô∏è" # No tests run
                  
                  time_str = f"{total_time:.2f}s"
                  summary_lines.append(f"| {status_icon} | {name} | {total_tests} | {time_str} |")
                  
              except Exception as e:
                  summary_lines.append(f"| ‚ö†Ô∏è | {name} | Error parsing | - |")
                  print(f"Error parsing {key}: {e}")

          summary_lines.append("")
          if all_passed:
              summary_lines.append("> ‚ú® **All systems operational.** Ready to merge!")
          else:
              summary_lines.append("> üö® **Failures detected.** Please review the logs.")

          with open("test_summary.md", "w", encoding="utf-8") as f:
              f.write("\n".join(summary_lines))
          EOF
          
          python3 generate_summary.py

      - name: Post Comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test_summary.md', 'utf8');
            
            // Signature to identify our comment
            const signature = '<!-- toon-py-test-summary -->';
            const body = `${signature}\n${summary}`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c => c.body.includes(signature));

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Publish Detailed Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: test-results/**/*.xml
          comment_mode: off 
          check_name: "Detailed Test Results"

  # Final status check - aggregates all test results
  test-status:
    name: ‚úÖ All Tests
    runs-on: ubuntu-latest
    needs: [smoke, test-matrix, benchmarks, fuzz]
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          FAILED=0
          
          if [ "${{ needs.smoke.result }}" == "success" ]; then
            echo "‚úÖ **Smoke Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Smoke Tests**: Failed" >> $GITHUB_STEP_SUMMARY
            FAILED=1
          fi
          
          if [ "${{ needs.test-matrix.result }}" == "success" ]; then
            echo "‚úÖ **Test Matrix** (Unit, Compliance, Security, Integration): Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Test Matrix**: Failed" >> $GITHUB_STEP_SUMMARY
            FAILED=1
          fi
          
          if [ "${{ needs.benchmarks.result }}" == "success" ]; then
            echo "‚úÖ **Benchmarks**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Benchmarks**: Failed" >> $GITHUB_STEP_SUMMARY
            FAILED=1
          fi
          
          if [ "${{ needs.fuzz.result }}" == "success" ]; then
            echo "‚úÖ **Fuzz Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Fuzz Tests**: Failed" >> $GITHUB_STEP_SUMMARY
            FAILED=1
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ $FAILED -eq 1 ]; then
            echo "### ‚ùå Some tests failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "### ‚úÖ All tests passed!" >> $GITHUB_STEP_SUMMARY
          fi

